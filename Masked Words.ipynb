{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xUu3dliPZQc"
      },
      "source": [
        "# **Masked Word Prediction**\n",
        "\n",
        "**Dataset decription.**\n",
        "The dataset consists of training and test files in plain text format (.txt). Both utilize a limited vocabulary of approximately 20 words. While the sentences don't necessarily follow proper English grammar, they share similar linguistic patterns and structure. The training set contains 10,000 complete sentences. The test set includes 30,000 sentences, each with exactly one word replaced by a `<mask>` token. The task is to develop a model that accurately predicts these masked words.\n",
        "\n",
        "**Overview of the notebook**\n",
        "Here is an overview of the steps needed for this task:\n",
        "1. *Data Loading.* \n",
        "2. *Data Preparation.* \n",
        "3. *Model Architecture.* \n",
        "4. *Training Implementation.* \n",
        "5. *Prediction.* "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Dk45SVmcR_U2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import random\n",
        "\n",
        "from collections import Counter # to build vocab easily"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hn4wUeU6Rlk8"
      },
      "source": [
        "### 1. Load the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tYQMOijb6CpZ"
      },
      "outputs": [],
      "source": [
        "# Load text file\n",
        "def load_text(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        text = f.read().lower()\n",
        "    return text\n",
        "\n",
        "train_data = load_text(\"train_data.txt\")\n",
        "test_data = load_text(\"test_data.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7dUjsoy9PWtc",
        "outputId": "8d187f79-11c8-4ec2-f6a2-f1e6c76c0562"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----EXAMPLE TRAINING SENTENCES----\n",
            "a cat chases on a small horse.\n",
            "this dog in the big park in the big park near the big bird chases near the small park.\n",
            "the big dog chases this park in the big park.\n",
            "the small dog near the big bird likes near the big dog.\n",
            "the car near this big park in the park near the small dog in the small park on the small dog is big.\n",
            "\n",
            "----EXAMPLE TEST SENTENCES----\n",
            "this big car is <mask>.\n",
            "the car in a car is big <mask> this <mask> is big.\n",
            "this big bird sees near this big bird near the big <mask> in a bird.\n",
            "a big bird in a <mask> cat chases a bird.\n",
            "the big <mask> likes this car.\n"
          ]
        }
      ],
      "source": [
        "# Load text file\n",
        "def load_text(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        text = f.read().lower()\n",
        "    return text\n",
        "\n",
        "train_data = load_text(\"train_data.txt\")\n",
        "test_data = load_text(\"test_data.txt\")\n",
        "\n",
        "# some examples\n",
        "c = 0\n",
        "print('----EXAMPLE TRAINING----')\n",
        "for sentence in train_data.split('.'):\n",
        "    print(sentence.strip() + '.')\n",
        "    c += 1\n",
        "    if c > 4:\n",
        "        break\n",
        "\n",
        "c = 0\n",
        "print('\\n----EXAMPLE TEST----')\n",
        "for sentence in test_data.split('.'):\n",
        "    print(sentence.strip() + '.')\n",
        "    c += 1\n",
        "    if c > 4:\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSOx1TioSan7"
      },
      "source": [
        "## 2. Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-bH05EOSPxd",
        "outputId": "7575732b-da85-40f0-8a09-70edf5db6c7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "vocabulary size is 22\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'a': 2,\n",
              " 'cat': 3,\n",
              " 'chases': 4,\n",
              " 'on': 5,\n",
              " 'small': 6,\n",
              " 'horse': 7,\n",
              " '.': 8,\n",
              " 'this': 9,\n",
              " 'dog': 10,\n",
              " 'in': 11,\n",
              " 'the': 12,\n",
              " 'big': 13,\n",
              " 'park': 14,\n",
              " 'near': 15,\n",
              " 'bird': 16,\n",
              " 'likes': 17,\n",
              " 'car': 18,\n",
              " 'is': 19,\n",
              " 'house': 20,\n",
              " 'sees': 21,\n",
              " '<mask>': 0,\n",
              " '<unk>': 1}"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Tokenization and Vocabulary - assuming everything is in the vocab from train \n",
        "# no option for pad or unknown token\n",
        "def build_vocab(text, min_freq=1):\n",
        "    tokens = text.split()\n",
        "    counter = Counter(tokens)\n",
        "    vocab = {word: idx for idx, (word, count) in enumerate(counter.items(), start=2) if count >= min_freq}\n",
        "    vocab[\"<mask>\"] = 0\n",
        "    vocab[\"<unk>\"] = 1\n",
        "    return vocab\n",
        "\n",
        "vocab = build_vocab(train_data)\n",
        "print(f'vocabulary size is {len(vocab)}')\n",
        "vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_OeMGpZcTWLc",
        "outputId": "bf25dd2d-e261-488c-9f3d-57b5018f4071"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[12, 13, 10, 21, 2, 6, 3, 8, 9, 20, 19, 1, 8]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# tokenizer\n",
        "class Tokenizer:\n",
        "    def __init__(self, vocab):\n",
        "        self.vocab = vocab\n",
        "        self.inv_vocab = {idx: word for word, idx in vocab.items()}\n",
        "\n",
        "    def encode(self, text):\n",
        "        text = text.lower().replace('.', ' .') # to ensure <mask>. can be spit\n",
        "        special_tokens = {\"<mask>\"}  # Ensure special tokens are not split\n",
        "        tokens = text.split()  # Use simple split() to preserve \"<mask>\" as a single token\n",
        "        return [self.vocab.get(token, vocab[\"<unk>\"]) for token in tokens]\n",
        "\n",
        "    def decode(self, tokens):\n",
        "        return ' '.join([self.inv_vocab.get(token, \"<unk>\") for token in tokens])\n",
        "\n",
        "tokenizer = Tokenizer(vocab)\n",
        "\n",
        "example_sentence = 'The big dog sees a small cat. This house is great.'\n",
        "\n",
        "encoded = tokenizer.encode(example_sentence)\n",
        "encoded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "G8OnarqYUo4P",
        "outputId": "e7753711-a442-4e31-9db0-91b1b0062d7b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'the big dog sees a small cat . this house is <unk> .'"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.decode(encoded)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sy1CDZ-kV5Bc"
      },
      "source": [
        "## 3. `DataLoader` for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ndR-aLBaVtQd"
      },
      "outputs": [],
      "source": [
        "# Masked Dataset Class \n",
        "import random\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class MaskedDataset(Dataset):\n",
        "    def __init__(self, text, vocab, seq_length=20, mask_prob=0.15):\n",
        "        self.vocab = vocab\n",
        "        self.tokens = tokenizer.encode(text)\n",
        "        self.seq_length = seq_length\n",
        "        self.mask_prob = mask_prob\n",
        "        self.mask_token_id = self.vocab[\"<mask>\"]  \n",
        "        self.vocab_size = len(vocab)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.tokens) - self.seq_length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get a sequence of length seq_length\n",
        "        tokens = self.tokens[idx: idx + self.seq_length]\n",
        "        masked_tokens, mask, actual_tokens = self.mask_tokens(tokens)\n",
        "        return (torch.tensor(masked_tokens, dtype=torch.long),\n",
        "                torch.tensor(mask, dtype=torch.long),\n",
        "                torch.tensor(actual_tokens, dtype=torch.long))\n",
        "\n",
        "    def mask_tokens(self, tokens):\n",
        "        \"\"\"\n",
        "        Applies the BERT-style 80/10/10 masking to 'tokens':\n",
        "          - With probability mask_prob, each token is chosen to be masked:\n",
        "            * 80% of the time -> replace with <mask>\n",
        "            * 10% of the time -> replace with a random token\n",
        "            * 10% of the time -> keep the original token\n",
        "        \"\"\"\n",
        "        masked_tokens = tokens.copy()\n",
        "        actual_tokens = tokens.copy()\n",
        "        mask = [0] * len(tokens)\n",
        "\n",
        "        for i in range(len(tokens)):\n",
        "            # Decide if we will mask this token\n",
        "            if random.random() < self.mask_prob:\n",
        "                mask[i] = 1\n",
        "\n",
        "                rand_num = random.random()\n",
        "                if rand_num < 0.8:\n",
        "                    # 80% replace with <mask>\n",
        "                    masked_tokens[i] = self.mask_token_id\n",
        "                elif rand_num < 0.9:\n",
        "                    # 10% replace with random token\n",
        "                    masked_tokens[i] = random.randint(0, self.vocab_size - 1)\n",
        "                else:\n",
        "                    # 10% keep the same token\n",
        "                    pass\n",
        "\n",
        "        return masked_tokens, mask, actual_tokens\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3JK36YEWbgL",
        "outputId": "ac5f99c5-7b25-4456-8f4e-e3582410f3ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "length of the dataset: 17, number of batches = 9\n",
            "torch.Size([2, 5]) torch.Size([2, 5]) torch.Size([2, 5])\n",
            "1st sample in batch : the big dog sees a\n",
            "2nd sample in batch : big dog sees a cat\n"
          ]
        }
      ],
      "source": [
        "example_data = \"the big dog sees a cat. the house near the park is big. the horse chases a small bird.\"\n",
        "\n",
        "dataset = MaskedDataset(example_data, vocab, seq_length=5, mask_prob=0.2)\n",
        "dataloader = DataLoader(dataset, batch_size=2, shuffle=False)\n",
        "print(f'length of the dataset: {len(dataset)}, number of batches = {len(dataloader)}')\n",
        "\n",
        "for masked_tokens, mask, actual_tokens in dataloader:\n",
        "    print(masked_tokens.shape, mask.shape, actual_tokens.shape)\n",
        "    print(f'1st sample in batch : {tokenizer.decode(actual_tokens[0].tolist())}')\n",
        "    print(f'2nd sample in batch : {tokenizer.decode(actual_tokens[1].tolist())}')\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVAkgzzeYtPF"
      },
      "source": [
        "Split into a train and validation set and set up the dataloaders."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8104LGoXZSe",
        "outputId": "b49791a9-31e1-46bf-830b-ce166187f101"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(107713, 6059, 674)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "seq_length = 10\n",
        "dataset = MaskedDataset(train_data, vocab, seq_length=seq_length)\n",
        "\n",
        "train_size = int(0.9 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
        "len(dataset), len(train_loader), len(val_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qtccITeZFRE"
      },
      "source": [
        "## 4. Training function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OptTVX0IXmps"
      },
      "outputs": [],
      "source": [
        "# train model\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, device, num_epochs=5):\n",
        "    model.to(device)\n",
        "    global_step = 0 \n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_train_loss = 0\n",
        "        correct_train = 0\n",
        "        total_train = 0\n",
        "\n",
        "        for masked_tokens, mask, actual_tokens in train_loader:\n",
        "            masked_tokens, mask, actual_tokens = (\n",
        "                masked_tokens.to(device),\n",
        "                mask.to(device),\n",
        "                actual_tokens.to(device)\n",
        "            )\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = model(masked_tokens)\n",
        "            loss = criterion(output.view(-1, output.size(-1)), actual_tokens.view(-1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Step the scheduler *every batch* rather than every epoch\n",
        "            scheduler.step()\n",
        "\n",
        "            global_step += 1\n",
        "\n",
        "            total_train_loss += loss.item()\n",
        "\n",
        "            # Calculate accuracy on masked tokens\n",
        "            predicted_tokens = output.argmax(dim=-1)\n",
        "            correct_train += ((predicted_tokens == actual_tokens) & (mask == 1)).sum().item()\n",
        "            total_train += mask.sum().item()\n",
        "\n",
        "        train_accuracy = (correct_train / total_train) * 100 if total_train > 0 else 0\n",
        "        avg_train_loss = total_train_loss / len(train_loader)\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        total_val_loss = 0\n",
        "        correct_val = 0\n",
        "        total_val = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for masked_tokens, mask, actual_tokens in val_loader:\n",
        "                masked_tokens, mask, actual_tokens = (\n",
        "                    masked_tokens.to(device),\n",
        "                    mask.to(device),\n",
        "                    actual_tokens.to(device)\n",
        "                )\n",
        "                output = model(masked_tokens)\n",
        "                loss = criterion(output.view(-1, output.size(-1)), actual_tokens.view(-1))\n",
        "                total_val_loss += loss.item()\n",
        "\n",
        "                predicted_tokens = output.argmax(dim=-1)\n",
        "                correct_val += ((predicted_tokens == actual_tokens) & (mask == 1)).sum().item()\n",
        "                total_val += mask.sum().item()\n",
        "\n",
        "        val_accuracy = (correct_val / total_val) * 100 if total_val > 0 else 0\n",
        "        avg_val_loss = total_val_loss / len(val_loader)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}, \"\n",
        "              f\"Train Loss: {avg_train_loss:.4f}, Train Acc: {train_accuracy:.2f}%, \"\n",
        "              f\"Val Loss: {avg_val_loss:.4f}, Val Acc: {val_accuracy:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tx4WoyEqZ8UK"
      },
      "source": [
        "## 5. Models and Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "SAs1ltWW4PSL"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, embed_dim, max_len=500):\n",
        "        super().__init__()\n",
        "\n",
        "        pe = torch.zeros(max_len, embed_dim)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-math.log(10000.0) / embed_dim))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        pe = pe.unsqueeze(0)  # Shape: (1, max_len, embed_dim)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "      return x + self.pe[:, :x.size(1), :]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bkTveNGwafff"
      },
      "outputs": [],
      "source": [
        "class WordPredictorTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_dim, num_layers, num_heads):\n",
        "        self.setting = {'name': 'WordPredictorTransformer',\n",
        "                        'vocab_size': vocab_size,\n",
        "                        'hidden_dim' : hidden_dim,\n",
        "                        'num_layers' : num_layers,\n",
        "                        'num_heads' : num_heads}\n",
        "        super(WordPredictorTransformer, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_dim)\n",
        "        self.position = PositionalEncoding(hidden_dim)\n",
        "        encoder = nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=num_heads, dropout=0.1, dim_feedforward=4 * hidden_dim, batch_first=True) \n",
        "        self.transformer = nn.TransformerEncoder(\n",
        "            encoder,\n",
        "            num_layers=num_layers\n",
        "        )\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x) # first the embedding layer\n",
        "        x = self.position(x)\n",
        "        attention_out = self.transformer(x)\n",
        "        out = self.fc(attention_out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "pOPrtgJHu00y"
      },
      "outputs": [],
      "source": [
        "from transformers import get_linear_schedule_with_warmup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "lY_14YY3bVGV",
        "outputId": "af775b36-3436-4683-9064-f1fedd52be34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "active device is cpu\n",
            "Epoch 1, Train Loss: 0.3252, Train Acc: 33.59%, Val Loss: 0.1313, Val Acc: 47.57%\n",
            "Epoch 2, Train Loss: 0.1315, Train Acc: 48.06%, Val Loss: 0.1282, Val Acc: 48.46%\n",
            "Epoch 3, Train Loss: 0.1284, Train Acc: 49.43%, Val Loss: 0.1204, Val Acc: 50.53%\n",
            "Epoch 4, Train Loss: 0.1252, Train Acc: 50.10%, Val Loss: 0.1192, Val Acc: 52.06%\n",
            "Epoch 5, Train Loss: 0.1225, Train Acc: 50.89%, Val Loss: 0.1153, Val Acc: 51.55%\n",
            "Epoch 6, Train Loss: 0.1217, Train Acc: 51.24%, Val Loss: 0.1177, Val Acc: 52.48%\n",
            "Epoch 7, Train Loss: 0.1198, Train Acc: 51.70%, Val Loss: 0.1166, Val Acc: 52.77%\n",
            "Epoch 8, Train Loss: 0.1195, Train Acc: 51.74%, Val Loss: 0.1146, Val Acc: 52.98%\n",
            "Epoch 9, Train Loss: 0.1178, Train Acc: 52.35%, Val Loss: 0.1172, Val Acc: 52.14%\n",
            "Epoch 10, Train Loss: 0.1173, Train Acc: 52.41%, Val Loss: 0.1170, Val Acc: 52.85%\n",
            "Epoch 11, Train Loss: 0.1160, Train Acc: 52.65%, Val Loss: 0.1135, Val Acc: 52.96%\n",
            "Epoch 12, Train Loss: 0.1158, Train Acc: 52.83%, Val Loss: 0.1154, Val Acc: 53.27%\n",
            "Epoch 13, Train Loss: 0.1147, Train Acc: 53.04%, Val Loss: 0.1115, Val Acc: 53.21%\n",
            "Epoch 14, Train Loss: 0.1146, Train Acc: 53.38%, Val Loss: 0.1119, Val Acc: 53.84%\n",
            "Epoch 15, Train Loss: 0.1147, Train Acc: 53.29%, Val Loss: 0.1120, Val Acc: 53.65%\n",
            "Epoch 16, Train Loss: 0.1139, Train Acc: 53.46%, Val Loss: 0.1119, Val Acc: 53.33%\n",
            "Epoch 17, Train Loss: 0.1135, Train Acc: 53.42%, Val Loss: 0.1144, Val Acc: 53.57%\n",
            "Epoch 18, Train Loss: 0.1128, Train Acc: 53.64%, Val Loss: 0.1109, Val Acc: 54.20%\n",
            "Epoch 19, Train Loss: 0.1124, Train Acc: 53.88%, Val Loss: 0.1121, Val Acc: 54.05%\n",
            "Epoch 20, Train Loss: 0.1125, Train Acc: 53.95%, Val Loss: 0.1132, Val Acc: 54.13%\n"
          ]
        }
      ],
      "source": [
        "vocab_size = len(vocab)\n",
        "# initiate model\n",
        "model_trans = WordPredictorTransformer(vocab_size, hidden_dim=512, num_layers=8, num_heads=8)\n",
        "# initiate loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model_trans.parameters(), lr=0.0001)\n",
        "num_epochs = 20\n",
        "num_training_steps = num_epochs * len(train_loader)\n",
        "num_warmup_steps = int(0.1 * num_training_steps)\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=num_warmup_steps,\n",
        "    num_training_steps=num_training_steps\n",
        ")\n",
        "# set up device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f'active device is {device}')\n",
        "# train\n",
        "train_model(model_trans, train_loader, val_loader, criterion, optimizer, scheduler, device, num_epochs=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CeOwshlvbwHP"
      },
      "outputs": [],
      "source": [
        "torch.save(model_trans.state_dict(), \"/content/model_transformer.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JT_KKgLuhb9t"
      },
      "outputs": [],
      "source": [
        "# initialize from the correct class with correct parameters\n",
        "model_trans = WordPredictorTransformer(vocab_size, hidden_dim=32, num_layers=2, num_heads=4)\n",
        "model_trans.load_state_dict(torch.load(\"/content/model_transformer.pth\", weights_only=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sq-nmbsqAv1l"
      },
      "source": [
        "Saved `setting` attribute to the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EkqJpjURA6L-"
      },
      "outputs": [],
      "source": [
        "def save_model(model, path='/content/model.pth'):\n",
        "    torch.save({\n",
        "        'state_dict': model.state_dict(),\n",
        "        'setting': model.setting  # Save variable parameters\n",
        "    }, path)\n",
        "\n",
        "save_model(model_trans, '/content/model_transformer.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6lL6AzcdG0Bk"
      },
      "outputs": [],
      "source": [
        "checkpoint = torch.load('/content/model_transformer.pth')\n",
        "checkpoint['setting']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ai-48YVtG8mi"
      },
      "source": [
        "Rebuild the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nmvr9LgjG_Ba"
      },
      "outputs": [],
      "source": [
        "model_new = WordPredictorTransformer(vocab_size, hidden_dim=32, num_layers=2, num_heads=4)\n",
        "model_new.load_state_dict(checkpoint['state_dict'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tiGudt50hlJq"
      },
      "source": [
        "## Predicting on the test data\n",
        "\n",
        "To predict masked words in the test data, segment it into sequences matching training sequence length, allowing for overlapping sections to ensure each mask is predicted once. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6GNFVCSiiUAN"
      },
      "outputs": [],
      "source": [
        "test_data = load_text(\"/content/test_data.txt\")\n",
        "sum([token==vocab[\"<mask>\"] for token in tokenizer.encode(test_data)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TvfHZQs-igyI"
      },
      "outputs": [],
      "source": [
        "from torch.nn.functional import softmax\n",
        "def predict_words(model, text, vocab, seq_length, device):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    tokenized = tokenizer.encode(text)\n",
        "    mask_token_id = vocab[\"<mask>\"]\n",
        "    target_size = sum([token==vocab[\"<mask>\"] for token in tokenized])\n",
        "    mask_positions = [i for i, token in enumerate(tokenized) if token == mask_token_id]\n",
        "    predicted_words = ['']*target_size\n",
        "    visited_positions = set()\n",
        "\n",
        "    # Process text in sliding windows to maintain context\n",
        "    step_size = seq_length // 2  # Overlapping step to retain context\n",
        "    for start in range(0, len(tokenized), step_size):\n",
        "        end = min(start + seq_length, len(tokenized))\n",
        "        chunk = tokenized[start:end]\n",
        "\n",
        "        # Check if there are masks in this chunk\n",
        "        local_mask_positions = [i for i in mask_positions if start <= i < end]\n",
        "        if not local_mask_positions:\n",
        "            continue  # Skip if no mask in this chunk\n",
        "\n",
        "        # Convert to tensor and move to device\n",
        "        input_tensor = torch.tensor([chunk]).to(device)\n",
        "\n",
        "        # Get model predictions\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_tensor)\n",
        "            logits = outputs[0]  # Shape: (seq_length, vocab_size)\n",
        "\n",
        "        # Convert logits to probabilities\n",
        "        probs = softmax(logits, dim=-1)\n",
        "\n",
        "        # Predict words for mask positions\n",
        "        for pos in local_mask_positions:\n",
        "            global_mask_index = mask_positions.index(pos)\n",
        "            relative_pos = pos - start  # Convert global index to local chunk index\n",
        "            predicted_token_id = torch.argmax(probs[relative_pos]).item()\n",
        "            predicted_token = tokenizer.decode([predicted_token_id])\n",
        "            if global_mask_index not in visited_positions: # ensures if prediciton already made, then skip\n",
        "                predicted_words[global_mask_index] = predicted_token\n",
        "                visited_positions.add(global_mask_index)\n",
        "\n",
        "    return predicted_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jpdfv0g5oFx9"
      },
      "outputs": [],
      "source": [
        "predicted = predict_words(model_new, text=test_data, vocab=vocab, seq_length=seq_length, device=device)\n",
        "len(predicted)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Z5Tonpupqr6"
      },
      "outputs": [],
      "source": [
        "print(f'first 5 predictions: {predicted[:5]}')\n",
        "print(f'last  5 predictions: {predicted[-5:]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zPPUnZ4Mo_dZ"
      },
      "outputs": [],
      "source": [
        "def save_csv(predicted_words, path='/content/', filename='predictions'):\n",
        "    import pandas as pd\n",
        "    assert len(predicted_words) == 30000 \n",
        "\n",
        "    df = pd.DataFrame({\n",
        "        'id': range(30000),\n",
        "        'prediction': predicted_words\n",
        "    })\n",
        "    df.to_csv(path+filename+'.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-K4NrQfuSmD"
      },
      "outputs": [],
      "source": [
        "save_csv(predicted, filename='basic_notebook') "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CalqmaYh9XBe"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
